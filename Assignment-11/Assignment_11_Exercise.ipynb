{"cells":[{"cell_type":"markdown","metadata":{"id":"vNaNrqSt0aY-"},"source":["# ECE 47300 Assignment 11 Exercise\n","\n","Your Name: Paloma Arellano"]},{"cell_type":"markdown","metadata":{"id":"CUr8IhHt0aZB"},"source":["### Objective: Build different transformer components and test them."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"eFiCv_bj0aZC","executionInfo":{"status":"ok","timestamp":1682102129564,"user_tz":240,"elapsed":7929,"user":{"displayName":"Paloma Arellano","userId":"14476098746200610980"}}},"outputs":[],"source":["import numpy as np\n","import string\n","import time\n","import torch\n","import pdb\n","import math\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.nn import functional as F\n","np.random.seed(124)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9EizHOQx0aZD"},"source":["### NOTE: In this assignment, we will use the convention of having the batch dimension first so tensors will have shapes of `(N, L, D)` where `N` is the batch dimension, `L` is the max sequence length, and `D` is the feature dimension.\n","The default in PyTorch is for the sequence dimension to be first, i.e., `(L, N, D)` but most functions in PyTorch can be altered to make the batch dimension to be first by using `batch_first=True`, see for example the arguments for [`torch.nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n"]},{"cell_type":"markdown","metadata":{"id":"9wibSKWf0aZE"},"source":["# Exercise 1: Positional Encoder (20 points)\n","\n","The positional encoder is a simple function that takes a 3D tensor of shape (batch_size, sequence_length, encoding_size), i.e., `(N, L, D)`, and returns a 3D tensor of the same shape where positional encoding embedding has been added. The positional encoder is a function of the position of the token in the sequence. The positional encoder is defined as:\n","\n","$$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{model}})$$\n","\n","$$PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{model}})$$\n","\n","where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.\n","\n","In practice, the positional encoding is added to the embedding vector. This is done by first creating a tensor of shape (1, sequence_length, d_model) and then adding it to the embedding vector. This ensures that the positional encoding is added to every element in the batch via broadcasting.\n","\n","Hints:\n","- If done correctly the output of the code below should look like:\n","```\n","torch.Size([1, 4, 512])\n","False\n","input_pe: tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500]) \n","output_pe: tensor([0.2263, 1.4525, 0.6788, 1.9051, 1.1314])\n","input_pe: tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500]) \n","output_pe: tensor([1.0677, 1.0222, 1.4808, 1.5285, 1.8931])\n","input_pe: tensor([5.0800, 5.0900, 5.1000, 5.1100, 5.1200]) \n","output_pe: tensor([115.9473, 115.1735, 116.3998, 115.6261, 116.8524])\n","```"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moQezSWO0aZF","executionInfo":{"status":"ok","timestamp":1682102132340,"user_tz":240,"elapsed":2781,"user":{"displayName":"Paloma Arellano","userId":"14476098746200610980"}},"outputId":"7ed49963-28cc-412d-98d6-35e8d0e3b383"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4, 512])\n","False\n","input_pe: tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500]) \n","output_pe: tensor([0.2263, 1.4525, 0.6788, 1.9051, 1.1314])\n","input_pe: tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500]) \n","output_pe: tensor([1.0677, 1.0222, 1.4808, 1.5285, 1.8931])\n","input_pe: tensor([5.0800, 5.0900, 5.1000, 5.1100, 5.1200]) \n","output_pe: tensor([115.9473, 115.1735, 116.3998, 115.6261, 116.8524])\n"]}],"source":["class PositionalEncoder(nn.Module):\n","    def __init__(self, d_model, max_seq_len = 80):\n","        super().__init__()\n","        self.d_model = d_model\n","        \n","        # create constant 'pe' matrix with values dependant on \n","        # pos and i\n","        pe = torch.zeros(max_seq_len, d_model)\n","\n","        #### YOUR CODE HERE ####\n","        # Loop over the positions and the embedding dimensions\n","        # and calculate the positional encoding for each dimension\n","        # and position\n","        # If you want extra challenge, try to do this without loops.\n","        for pos in range(max_seq_len):\n","          for i in range(0, d_model, 2):\n","            pe[pos, i] =      math.sin(pos / (10000 ** ((2 * i) / d_model)))\n","            pe[pos, i + 1] =  math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n","\n","                \n","        #### END YOUR CODE ####\n","        \n","        pe = pe.unsqueeze(0)\n","        # Register this as something to keep when saving a model\n","        #  but that is not a learnable parameter\n","        self.register_buffer('pe', pe)\n"," \n","    \n","    def forward(self, x):\n","        # make embeddings relatively larger than pe\n","        x = x * math.sqrt(self.d_model)\n","        # add constant positional encoding to embedding\n","        seq_len = x.size(1)\n","        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)\n","        return x\n","\n","pe = PositionalEncoder(512)\n","input_pe = torch.arange(1, 513)*0.01\n","input_pe = input_pe.repeat(1, 4, 1).float()\n","\n","output_pe = pe(input_pe)\n","print(output_pe.shape)\n","\n","# check the difference between the two embeddings\n","print(torch.equal(input_pe, output_pe)) # They should not be equal after adding positional encoding\n","print(f\"input_pe: {input_pe[0, 0, 0:5]} \\noutput_pe: {output_pe[0, 0, 0:5]}\")\n","print(f\"input_pe: {input_pe[0, 1, 0:5]} \\noutput_pe: {output_pe[0, 1, 0:5]}\")\n","print(f\"input_pe: {input_pe[0, 2, -5:]} \\noutput_pe: {output_pe[0, 2, -5:]}\")"]},{"cell_type":"markdown","metadata":{"id":"ABNASIYx0aZG"},"source":["# Exercise 2: Scaled Dot-Product Attention (30 points)\n","In this exercise, you will implement a version of attention used in transformers. The key difference from the one described in class is that the attention scores (pre-softmax) are scaled by a factor of $\\frac{1}{\\sqrt{d_k}}$, where $d_k$ is the dimension of the keys (and the queries):\n","$$\n","A(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n","$$\n","where $Q \\in \\mathbb{R}^{L\\times d_k}$, $K \\in \\mathbb{R}^{L \\times d_k}$, and $V \\in \\mathbb{R}^{L \\times d_v}$, where $d_v$ is the dimension of the values. The softmax is across the column dimension. The output of attention should be a matrix $A \\in \\mathbb{R}^{L \\times d_v}$.\n","\n","Additionally, we will implement a **batched version** of this that can be used for multiple sequences at the same time. To do this, you will need to use the following **batched** version(s) of matrix multiplication either [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html) or [`torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html) for both the $QK^T$ and the product of the attention matrix and $V$. We recommend that you use `torch.bmm` as it is more explicit.\n","\n","Hint:\n","- You will need to transpose the matrices in the `k` tensor. Specifically, you will need to swap the last and second to last dimension so that it has shape (batch_size, d_k, seq_len). One way to do this is via the `transpose` function.\n","- If done correctly, the output should be like below:\n","```\n","Output check (first): \n","tensor([-1.3709, -0.6827,  0.3234,  0.8677, -0.1474, -0.9653, -0.7344,  0.8126,\n","         0.1219,  0.3224,  0.6257, -0.0958, -0.1664, -0.0667, -0.2810,  0.3068,\n","        -0.7030, -0.6719,  0.4364, -1.0071,  0.3534,  0.3160,  0.0326, -0.7315,\n","        -0.5165])\n","Output check (last): \n","tensor([-0.2094,  1.3784,  0.2855, -0.1716,  0.1597, -0.6656,  0.3981, -0.9903,\n","        -0.6043, -0.6398,  0.0563, -1.5367, -0.0225, -0.8317,  0.0572,  0.2014,\n","         0.1324, -0.4563,  0.3832,  0.1051,  0.0653, -0.2076,  0.6225, -0.4946,\n","        -0.2935])\n","```"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RJrj5UTE0aZH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682102132341,"user_tz":240,"elapsed":12,"user":{"displayName":"Paloma Arellano","userId":"14476098746200610980"}},"outputId":"cc54f01b-807b-47ca-ebf7-eb9018a6dc0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Is shape of output correct? True\n","Is shape of att_values correct? True\n","Do attention values sum to 1? True\n","Output check (first): \n","tensor([-1.3709, -0.6827,  0.3234,  0.8677, -0.1474, -0.9653, -0.7344,  0.8126,\n","         0.1219,  0.3224,  0.6257, -0.0958, -0.1664, -0.0667, -0.2810,  0.3068,\n","        -0.7030, -0.6719,  0.4364, -1.0071,  0.3534,  0.3160,  0.0326, -0.7315,\n","        -0.5165])\n","Output check (last): \n","tensor([-0.2094,  1.3784,  0.2855, -0.1716,  0.1597, -0.6656,  0.3981, -0.9903,\n","        -0.6043, -0.6398,  0.0563, -1.5367, -0.0225, -0.8317,  0.0572,  0.2014,\n","         0.1324, -0.4563,  0.3832,  0.1051,  0.0653, -0.2076,  0.6225, -0.4946,\n","        -0.2935])\n"]}],"source":["def attention(q, k, v):\n","    '''\n","    Inputs:\n","    q: query vector of shape (batch_size, seq_len, d_k)\n","    k: key vector of shape (batch_size, seq_len, d_k)\n","    v: value vector of shape (batch_size, seq_len, d_v)\n","\n","    Returns:\n","    output: attention weighted sum of the value vectors \n","        of shape (batch_size, seq_len, d_k)\n","    scores: attention weights of shape (batch_size, seq_len, seq_len)\n","    '''        \n","    d_k = k.size(-1)\n","    assert d_k == q.size(-1), 'q and k should have the same dimensionality'\n","    d_v = v.size(-1)\n","\n","    #### YOUR CODE HERE ####\n","    #print(k.size())\n","    k = k.transpose(-2, -1)\n","    #print(k.size())\n","    att_values = torch.matmul(q, k)\n","    att_values = att_values / math.sqrt(d_k)\n","    att_values = F.softmax(att_values, dim=-1)\n","\n","    output = torch.matmul(att_values, v)\n","\n","    #### END YOUR CODE ####\n","\n","    return output, att_values\n","\n","# test the attention function with some random values\n","torch.manual_seed(42)  # Do not change random seed\n","q = torch.randn(2, 5, 512)\n","k = torch.randn(2, 5, 512)\n","v = torch.randn(2, 5, 256)\n","\n","output, att_values = attention(q, k, v)\n","print(f\"Is shape of output correct? {output.shape == v.shape}\")\n","print(f\"Is shape of att_values correct? {att_values.shape == torch.Size([q.shape[0], q.shape[1], q.shape[1]])}\")\n","print(f\"Do attention values sum to 1? {torch.allclose(torch.sum(att_values, dim=-1), torch.ones(1))}\")\n","\n","# Last 25 values of last sample and last token\n","print(f\"Output check (first): \\n{output[0,0,:25]}\")\n","print(f\"Output check (last): \\n{output[-1,-1,-25:]}\")\n"]},{"cell_type":"markdown","metadata":{"id":"wDP-dP0G0aZH"},"source":["# Exercise 3: Attention modules (50 points)\n","\n","## Task 1: Self-attention module\n","Implement a self-attention module that takes in `x` and computes `q`,`k`,`v` internally using 3 linear layers. Then, use your function from above to compute the output and attention and return it. The attention module should take as constructor parameters the `input_dim`, `key_dim`, and the `output_dim`.\n","\n","Your output should look like the following:\n","```\n","input shape: torch.Size([4, 10, 512])\n","output shape: torch.Size([4, 10, 512])\n","Input: \n","tensor([-0.1988, -0.3060,  0.6383,  0.5713,  1.2769])\n","Output: \n","tensor([ 8.6836e-02, -4.1725e-02, -2.5798e-01, -9.6712e-02,  3.0079e-05],\n","       grad_fn=<SliceBackward>)\n","\n","Does the module exhibit permutation-equivaraince? True\n","The following two lines should be the same:\n","tensor([ 0.1849,  0.3706,  0.1488,  0.0898,  0.2528,  0.4277,  0.4572,  0.1172,\n","         0.0893, -0.2081], grad_fn=<SliceBackward>)\n","tensor([ 0.1849,  0.3706,  0.1488,  0.0898,  0.2528,  0.4277,  0.4572,  0.1172,\n","         0.0893, -0.2081], grad_fn=<SliceBackward>)\n","```"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Gu2LjoC00aZI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682102631346,"user_tz":240,"elapsed":442,"user":{"displayName":"Paloma Arellano","userId":"14476098746200610980"}},"outputId":"e71a26c5-257d-4df1-b80a-b282478622d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["input shape: torch.Size([4, 10, 512])\n","output shape: torch.Size([4, 10, 512])\n","Input: \n","tensor([-0.1988, -0.3060,  0.6383,  0.5713,  1.2769])\n","Output: \n","tensor([ 8.6836e-02, -4.1725e-02, -2.5798e-01, -9.6712e-02,  3.0022e-05],\n","       grad_fn=<SliceBackward0>)\n","\n","Does the module exhibit permutation-equivaraince? True\n","The following two lines should be the same:\n","tensor([ 0.1849,  0.3706,  0.1488,  0.0898,  0.2528,  0.4277,  0.4572,  0.1172,\n","         0.0893, -0.2081], grad_fn=<SliceBackward0>)\n","tensor([ 0.1849,  0.3706,  0.1488,  0.0898,  0.2528,  0.4277,  0.4572,  0.1172,\n","         0.0893, -0.2081], grad_fn=<SliceBackward0>)\n"]}],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, input_dim, key_dim, output_dim):\n","        super().__init__()\n","        #### YOUR CODE HERE ####\n","        self.q = nn.Linear(input_dim, key_dim)\n","        self.k = nn.Linear(input_dim, key_dim)\n","        self.v = nn.Linear(input_dim, output_dim)\n","        self.output_dim = output_dim\n","\n","        #### END YOUR CODE ####\n","\n","    def forward(self, x):\n","        '''\n","        `x` has shape (batch_dim, sequence_length, input_dim) or (N, L, D_in)\n","        \n","        The output should have shape (batch_dim, sequence_length, output_dim) or (N, L, D_out).\n","        '''\n","        #### YOUR CODE HERE ####\n","        q = self.q(x)\n","        k = self.k(x)\n","        v = self.v(x)\n","\n","        output, att_values = attention(q, k, v)\n","        \n","        #### END YOUR CODE ####\n","\n","        return output\n","    \n","# test the self-attention module with some random values\n","torch.manual_seed(48)\n","input_dim = 512\n","key_dim = 64\n","output_dim = 512\n","self_attn = SelfAttention(input_dim, key_dim, output_dim)\n","x = torch.randn(4, 10, 512)\n","output = self_attn(x)\n","print(f\"input shape: {x.shape}\")\n","print(f\"output shape: {output.shape}\")\n","print(f'Input: \\n{x[0,0,:5]}\\nOutput: \\n{output[0,0,:5]}\\n')\n","\n","# For self-attention, let’s check the “permutation-equivariant” property, \n","# i.e., permute the input sequence and check if the output sequence is also permuted but otherwise the same. \n","# This is a nice sanity check that self-attention is working properly.\n","random_permutation = torch.randperm(x.size(1))\n","reverse_permutation = torch.zeros_like(random_permutation)\n","reverse_permutation[random_permutation] = torch.arange(len(random_permutation))\n","assert torch.all(x[:, random_permutation, :][:, reverse_permutation, :] == x), 'inverse is incorrect'\n","\n","x_prime = x[:, random_permutation, :] # Permute input\n","output_prime = self_attn(x_prime)\n","output_prime_permuted = output_prime[:, reverse_permutation, :]  # Reverse permutation of output\n","print(f'Does the module exhibit permutation-equivaraince?'\n","      f' {torch.allclose(output, output_prime_permuted, atol=1e-5, rtol=1)}')\n","print(f'The following two lines should be the same:')\n","print(output[-1,-1,:10])\n","print(output_prime_permuted[-1,-1,:10])"]},{"cell_type":"markdown","metadata":{"id":"QSzw0leT0aZI"},"source":["# Task 2: Cross Attention module\n","For cross attention, there will be an first input `x` that will correspond to the query and a second input `y` that will correspond to the keys and values. (In self-attention, `x` and `y` were equal).\n","This should be the same basic idea except that there is a linear layer to compute `q` from `x` and linear layers to compute `k` and `v` from `y`.\n","\n","The output should look like the following:\n","```\n","input shape x and y: torch.Size([3, 10, 512]), torch.Size([3, 10, 256])\n","output shape: torch.Size([3, 10, 128])\n","x\n","tensor([-0.0385,  0.9773, -1.4370,  0.8719, -2.1034, -0.2877,  0.3034, -1.9151,\n","         1.1799,  0.6151])\n","y\n","tensor([-2.1565,  0.2397,  0.5872,  0.3950, -0.6114,  0.3489, -0.3467,  0.2792,\n","        -1.2541,  0.4053])\n","output\n","tensor([-0.1544,  0.0847,  0.2329,  0.0549, -0.1424,  0.0711,  0.0105, -0.2139,\n","        -0.0208, -0.1942], grad_fn=<SliceBackward>)\n","```"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gQDqtEZ80aZJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682102132342,"user_tz":240,"elapsed":8,"user":{"displayName":"Paloma Arellano","userId":"14476098746200610980"}},"outputId":"3f6ceec2-5bde-467e-d405-f8abb07dc972"},"outputs":[{"output_type":"stream","name":"stdout","text":["input shape x and y: torch.Size([3, 10, 512]), torch.Size([3, 10, 256])\n","output shape: torch.Size([3, 10, 128])\n","x\n","tensor([-0.0385,  0.9773, -1.4370,  0.8719, -2.1034, -0.2877,  0.3034, -1.9151,\n","         1.1799,  0.6151])\n","y\n","tensor([-2.1565,  0.2397,  0.5872,  0.3950, -0.6114,  0.3489, -0.3467,  0.2792,\n","        -1.2541,  0.4053])\n","output\n","tensor([-0.1544,  0.0847,  0.2329,  0.0549, -0.1424,  0.0711,  0.0105, -0.2139,\n","        -0.0208, -0.1942], grad_fn=<SliceBackward0>)\n"]}],"source":["class CrossAttention(nn.Module):\n","    def __init__(self, x_input_dim, y_input_dim, key_dim, output_dim):\n","        super().__init__()\n","        #### YOUR CODE HERE ####\n","        self.q = nn.Linear(x_input_dim, key_dim)\n","        self.k = nn.Linear(y_input_dim, key_dim)\n","        self.v = nn.Linear(y_input_dim, output_dim)\n","\n","        #### END YOUR CODE ####\n","\n","    def forward(self, x, y):\n","        #### YOUR CODE HERE ####\n","        q = self.q(x)\n","        k = self.k(y)\n","        v = self.v(y)\n","\n","        output, att_values = attention(q, k, v)\n","\n","        #### END YOUR CODE ####\n","        \n","        return output\n","\n","# test the attention module with some random values\n","torch.manual_seed(14)\n","x_input_dim = 512\n","y_input_dim = 256\n","key_dim = 64\n","output_dim = 128\n","cross_attn = CrossAttention(x_input_dim, y_input_dim, key_dim, output_dim)\n","x = torch.randn(3, 10, x_input_dim)\n","y = torch.randn(3, 10, y_input_dim)\n","output = cross_attn(x, y)\n","print(f\"input shape x and y: {x.shape}, {y.shape}\")\n","print(f\"output shape: {output.shape}\")\n","print(f'x\\n{x[0,0,:10]}')\n","print(f'y\\n{y[0,0,:10]}')\n","print(f'output\\n{output[0,0,:10]}')"]},{"cell_type":"markdown","metadata":{"id":"nh6VseK_0aZJ"},"source":["## Task 3: Multi-headed self-attention module\n","Multi-headed self-attention merely passes the the input to each attention module, concatenates all the outputs, and then applies a linear layer to get the final output.\n","Implement multi-headed attention below.\n","\n","Output should look like:\n","```\n","input shape: torch.Size([3, 10, 256])\n","output shape: torch.Size([3, 10, 32])\n","x\n","tensor([ 0.7195, -0.3636,  1.3771,  0.3482, -0.0604, -0.3034, -0.0698,  0.2131,\n","        -0.9736, -0.4651])\n","output\n","tensor([ 0.2290, -0.0350,  0.0918, -0.1069, -0.1679, -0.1939, -0.2167,  0.1841,\n","         0.0546,  0.1668], grad_fn=<SliceBackward>)\n","```"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"L_6D_qX-0aZJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682104274471,"user_tz":240,"elapsed":20,"user":{"displayName":"Paloma Arellano","userId":"14476098746200610980"}},"outputId":"f92d5d3a-30c4-4692-9c45-891baa41c6e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["input shape: torch.Size([3, 10, 256])\n","output shape: torch.Size([3, 10, 32])\n","x\n","tensor([ 0.7195, -0.3636,  1.3771,  0.3482, -0.0604, -0.3034, -0.0698,  0.2131,\n","        -0.9736, -0.4651])\n","output\n","tensor([ 0.2290, -0.0350,  0.0918, -0.1069, -0.1679, -0.1939, -0.2167,  0.1841,\n","         0.0546,  0.1668], grad_fn=<SliceBackward0>)\n"]}],"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, attn_modules, final_output_dim):\n","        super().__init__()\n","        #### YOUR CODE HERE ####\n","        # 1) Save the attn_modules as a nn.ModuleList\n","        # 2) Setup a linear layer \n","        #   Hint for 2: Need to compute the concatenated dimensionality to \n","        #   setup linear layer by extracting the output dimension from each \n","        #   attention module.\n","        self.mod = nn.ModuleList(attn_modules)\n","\n","        lin_in = 0\n","        for i in self.mod:\n","          lin_in += i.output_dim\n","\n","        self.lin = nn.Linear(lin_in, final_output_dim)\n","        \n","        #### END YOUR CODE ####\n","\n","    def forward(self, x):\n","        #### YOUR CODE HERE ####\n","        # 1) Concatenate outputs of each self-attention module\n","        # 2) Apply final linear layer\n","\n","        out_list = []\n","        for i in self.mod:\n","          out_list.append(i(x))\n","        out_list = torch.cat(out_list, -1)\n","\n","        output = self.lin(out_list)\n","\n","        #### END YOUR CODE ####\n","\n","        return output\n","    \n","# test the multi-headed attention module with some random values\n","torch.manual_seed(10)\n","input_dim = 256\n","key_dim = 128 \n","output_dim = 64 \n","final_output_dim = 32\n","num_heads = 8 \n","attn_modules = [SelfAttention(input_dim, key_dim, output_dim//num_heads) for _ in range(num_heads)]\n","multi_attn = MultiHeadedAttention(attn_modules, final_output_dim)\n","\n","x = torch.randn(3, 10, input_dim)\n","output = multi_attn(x)\n","print(f\"input shape: {x.shape}\")\n","print(f\"output shape: {output.shape}\")\n","print(f'x\\n{x[0,0,:10]}\\noutput\\n{output[0,0,:10]}')\n"]},{"cell_type":"markdown","metadata":{"id":"JUNmXr7Q0aZK"},"source":["# (Optional, ungraded) Masked attention module \n","Try to implement the masked attention module for the decoder in a transformer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SZbaK0H0aZK"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}